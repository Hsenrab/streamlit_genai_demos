import openai
import os
import streamlit as st
import urllib.request
from azure.ai.projects import AIProjectClient
from azure.ai.agents.models import ListSortOrder
from azure.identity import DefaultAzureCredential

from dotenv import load_dotenv

load_dotenv()

client = openai.AzureOpenAI(
  azure_endpoint = os.getenv("OPENAI_API_ENDPOINT"), 
  api_key=os.getenv("OPENAI_API_KEY"),  
  api_version="2024-02-01"
)

def question(prompt, system_prompt="You are a helpful assistant."):
    """
    Generates a response from the OpenAI GPT-4o model based on a user prompt and an optional system prompt.
    Args:
        prompt (str): The user's input or question to be sent to the language model.
        system_prompt (str, optional): The system-level instruction or context for the assistant. Defaults to "You are a helpful assistant.".
    Returns:
        str: The content of the model's response to the user's prompt.
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message.content
    
    
def chat(prompt, history, response_format=None):
    """
    Sends a chat prompt along with conversation history to the OpenAI GPT-4o model and returns the assistant's response.
    Args:
        prompt (str): The latest user input to be sent to the model.
        history (list): A list of previous message dictionaries, each containing 'role' and 'content' keys, representing the conversation history.
        response_format (str, optional): The format for the response. If set to 'json', the API will be instructed
                                         to return a JSON object but the function will still return a string. 
                                         Defaults to None (plain text).
    Returns:
        str: The content of the model's response as a string, or an error message if the request fails.
    Raises:
        None: All exceptions are handled within the function.
    """
    
    messages=history + [{"role": "user", "content": prompt}]
    
    # Set up the API parameters
    params = {
        "model": "gpt-4o",
        "messages": messages
    }
    
    # Add response format if specified
    if response_format == 'json':
        params["response_format"] = {"type": "json_object"}
    
    with st.spinner("Waiting for response..."):
        try:
            response = client.chat.completions.create(**params)
            return response.choices[0].message.content
            
        except urllib.error.HTTPError as error:
            print("The request failed with status code: " + str(error.code))
            print(error.info())
            print(error.read().decode("utf8", 'ignore'))
            return "Sorry, I am unable to process your request at the moment. The request failed with status code: " + str(error.code)


def generate_markdown(image_url):
    """
    Extracts text content, especially tables, from an image using the GPT-4o model via OpenAI's API.
    The function sends a system prompt instructing the model to extract text and tables from the provided image URL.
    The response is expected to start with the page number and include any detected tables in markdown format,
    followed by a summary or analysis of the extracted content.
    Args:
        image_url (str): The URL of the image from which to extract text and tables.
    Returns:
        str: The extracted content in markdown format, as generated by the GPT-4o model.
    
    """
    
    system_prompt = """
    You are an AI assistance that extracts text from the image. You are especially good at extracting tables.
    Start your response with the page number of the image. 
    If the page contains images extract the text from the image and give a breif descriptioin of the image.
    Example Page:
    
    Page 123
    
    Monthly Savings
    | Month    | Savings |Details      |
    | -------- | ------- |------------ |
    | January  | $250    | for holiday |
    | February | $80     | pension     |
    | March    | $420    | new cat     |
    
    Savings were significantly lower in February. This is surprising because it is a short month and contributing to pension shuld be a priority.
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": system_prompt,
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Extract text from the image"},
                    {
                        "type": "image_url",
                        "image_url": {"url": image_url},
                    },
                ],
            },
        ],
        max_tokens=2000,
        temperature=0.0,
    )
    
    return response.choices[0].message.content


def summarize(markdown):
    """
    Summarizes the given markdown text using an AI assistant.
    Args:
        markdown (str): The markdown text to be summarized.
    Returns:
        str: The summarized version of the input markdown text, generated by the AI assistant.
    Notes:
        - Uses a system prompt from Streamlit session state with the key "summarize_prompt".
        - Relies on the `question` function to interact with the AI assistant.
    """
    
    system_prompt = st.session_state.get("summarize_prompt", "You are an AI assistant that summarizes markdown text")
    prompt = f"Input:\n{markdown}"

    return question(prompt, system_prompt)


def compare(markdown1, markdown2):
    """
    Compares two markdown documents using an AI assistant.
    Args:
        markdown1 (str): The first markdown document to compare.
        markdown2 (str): The second markdown document to compare.
    Returns:
        str: The AI-generated comparison result between the two markdown documents.
    Notes:
        - Uses a system prompt from Streamlit session state with the key "comparison_prompt", or a default prompt if not set.
        - Relies on the `question` function to interact with the AI assistant.
    """
    
    system_prompt = st.session_state.get("comparison_prompt", "You are an AI assistant that compares two markdown documents")
    prompt = f"Input:\n\n--- Start of Document 1 ---\n{markdown1}\n--- End of Document 1 ---\n\n--- Start of Document 2 ---\n{markdown2}\n--- End of Document 2 ---"

    return question(prompt, system_prompt)

def ai_foundry_web_search(prompt, history):
    """
    Sends a chat prompt along with conversation history to the AI Foundry agent for web search
    and returns the assistant's response.
    
    Uses the Azure AI Projects API to communicate with AI Foundry agents.
    
    Args:
        prompt (str): The latest user input to be sent to the agent.
        history (list): A list of previous message dictionaries, each containing 'role' and 'content' keys,
                       representing the conversation history.
    
    Returns:
        str: The content of the agent's response as a string, or an error message if the request fails.
    """    # Get configuration from environment variables
    ai_foundry_endpoint = os.getenv("AI_FOUNDRY_ENDPOINT")
    ai_foundry_agent_id = os.getenv("AI_FOUNDRY_AGENT_ID")
    
    # Check for required environment variables
    missing_vars = []
    if not ai_foundry_endpoint:
        missing_vars.append("AI_FOUNDRY_ENDPOINT")
    if not ai_foundry_agent_id:
        missing_vars.append("AI_FOUNDRY_AGENT_ID")
    
    if missing_vars:
        return f"Error: AI Foundry configuration is missing. Please check environment variables: {', '.join(missing_vars)}"
    
    with st.spinner("Searching the web for an answer..."):
        try:
            # Initialize the AI Project client
            project = AIProjectClient(
                credential=DefaultAzureCredential(),
                endpoint=ai_foundry_endpoint
            )
            
            # Get the agent
            agent = project.agents.get_agent(ai_foundry_agent_id)
            
            # Create a new thread if needed or use existing thread_id from session state
            if "ai_foundry_thread_id" not in st.session_state:
                thread = project.agents.threads.create()
                st.session_state.ai_foundry_thread_id = thread.id
            
            thread_id = st.session_state.ai_foundry_thread_id
            
            # Create a new message with the current prompt
            message = project.agents.messages.create(
                thread_id=thread_id,
                role="user",
                content=prompt
            )
            
            # Run the agent with the message
            run = project.agents.runs.create_and_process(
                thread_id=thread_id,
                agent_id=agent.id
            )
            
            # Check run status
            if run.status == "failed":
                return f"Run failed: {run.last_error}"
            
            # Get messages from the thread
            messages = project.agents.messages.list(
                thread_id=thread_id, 
                order=ListSortOrder.ASCENDING
            )
            
            # Format and display the messages
            response = ""
            for message in messages:
                if message.role == "assistant" and hasattr(message, 'text_messages') and message.text_messages:
                    response = message.text_messages[-1].text.value
            
            if not response:
                return "No response received from the AI Foundry agent."
                
            return response
                
        except Exception as e:
            return f"Error occurred while communicating with AI Foundry agent: {str(e)}"

# TODO create button to delete all uploaded pdfs and images.